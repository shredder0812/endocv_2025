MÃ´ táº£ chi tiáº¿t + code: tÃ­ch há»£p quá»¹ Ä‘áº¡o áº£o vÃ o KalmanFilterXYSR

DÆ°á»›i Ä‘Ã¢y mÃ¬nh cung cáº¥p má»™t hÆ°á»›ng triá»ƒn khai Ä‘áº§y Ä‘á»§ (lÃ½ thuyáº¿t â†’ cÃ´ng thá»©c â†’ code Python) Ä‘á»ƒ báº¡n cáº¯m tháº³ng vÃ o pipeline XYSR cá»§a báº¡n. Ná»™i dung gá»“m:

CÃ¡c phÃ©p biáº¿n Ä‘á»•i toÃ¡n â†’ code (s,r â†” w,h).

HÃ m ná»™i suy táº¡o danh sÃ¡ch bounding boxes áº£o (x, y, s, r) vá»›i cÃ´ng thá»©c tuyáº¿n tÃ­nh (ná»™i suy w,h).

HÃ m Ã¡p dá»¥ng quá»¹ Ä‘áº¡o áº£o vÃ o Kalman filter (apply_virtual_trajectory) â€” thá»±c hiá»‡n: cáº­p nháº­t KF tá»«ng bÆ°á»›c, dÃ¹ng confidence cho measurement áº£o, optional appearance check (cosine sim) Ä‘á»ƒ quyáº¿t Ä‘á»‹nh cÃ³ â€œxuáº¥tâ€ cÃ¡c há»™p áº£o vÃ o káº¿t quáº£ tracking hay khÃ´ng.

Gá»£i Ã½ chá»‰nh unfreeze() / update() trong class cá»§a báº¡n Ä‘á»ƒ dÃ¹ng confidence khi update áº£o.

Kiá»ƒm tra vÃ  tham sá»‘ tuning.

MÃ¬nh ghi ráº¥t rÃµ cÃ´ng thá»©c toÃ¡n trong comment Ä‘á»ƒ báº¡n dá»… theo dÃµi.

1) Chuyá»ƒn Ä‘á»•i s,r â†” w,h (ToÃ¡n â†’ code)

CÃ´ng thá»©c:

tá»« s, r sang w, h:

ğ‘¤
=
ğ‘ 
â‹…
ğ‘Ÿ
w=
sâ‹…r
	â€‹


â„
=
ğ‘ 
/
ğ‘Ÿ
h=
s/r
	â€‹


tá»« w, h sang s, r:

ğ‘ 
=
ğ‘¤
â‹…
â„
s=wâ‹…h

ğ‘Ÿ
=
ğ‘¤
/
â„
r=w/h

Code:

import numpy as np

_eps = 1e-6

def s_r_to_w_h(s: float, r: float):
    """Convert scale s (area) and ratio r to width w and height h."""
    # numeric safety
    s = float(s)
    r = float(max(r, _eps))
    w = np.sqrt(max(s * r, 0.0))
    h = np.sqrt(max(s / r, 0.0))
    return w, h

def w_h_to_s_r(w: float, h: float):
    """Convert width and height to scale s and ratio r."""
    w = float(max(w, _eps))
    h = float(max(h, _eps))
    s = w * h
    r = w / h
    return s, r

2) Ná»™i suy tuyáº¿n tÃ­nh cho quá»¹ Ä‘áº¡o áº£o (cÃ´ng thá»©c + code)

Giáº£ sá»­ cÃ³ hai quan sÃ¡t:

ğ‘§
1
=
(
ğ‘¥
1
,
ğ‘¦
1
,
ğ‘ 
1
,
ğ‘Ÿ
1
)
z
1
	â€‹

=(x
1
	â€‹

,y
1
	â€‹

,s
1
	â€‹

,r
1
	â€‹

) táº¡i frame t1

ğ‘§
2
=
(
ğ‘¥
2
,
ğ‘¦
2
,
ğ‘ 
2
,
ğ‘Ÿ
2
)
z
2
	â€‹

=(x
2
	â€‹

,y
2
	â€‹

,s
2
	â€‹

,r
2
	â€‹

) táº¡i frame t2

time_gap = t2 - t1 (sá»‘ bÆ°á»›c giá»¯a hai quan sÃ¡t; náº¿u =1 thÃ¬ khÃ´ng cÃ³ frame máº¥t)

Chi tiáº¿t ná»™i suy:

Chuyá»ƒn 
ğ‘ 
ğ‘–
,
ğ‘Ÿ
ğ‘–
â†’
ğ‘¤
ğ‘–
,
â„
ğ‘–
s
i
	â€‹

,r
i
	â€‹

â†’w
i
	â€‹

,h
i
	â€‹


TÃ­nh delta tá»«ng bÆ°á»›c:

ğ‘‘
ğ‘¥
=
ğ‘¥
2
âˆ’
ğ‘¥
1
ğ‘¡
ğ‘–
ğ‘š
ğ‘’
_
ğ‘”
ğ‘
ğ‘
dx=
time_gap
x
2
	â€‹

âˆ’x
1
	â€‹

	â€‹


ğ‘‘
ğ‘¦
=
ğ‘¦
2
âˆ’
ğ‘¦
1
ğ‘¡
ğ‘–
ğ‘š
ğ‘’
_
ğ‘”
ğ‘
ğ‘
dy=
time_gap
y
2
	â€‹

âˆ’y
1
	â€‹

	â€‹


ğ‘‘
ğ‘¤
=
ğ‘¤
2
âˆ’
ğ‘¤
1
ğ‘¡
ğ‘–
ğ‘š
ğ‘’
_
ğ‘”
ğ‘
ğ‘
dw=
time_gap
w
2
	â€‹

âˆ’w
1
	â€‹

	â€‹


ğ‘‘
â„
=
â„
2
âˆ’
â„
1
ğ‘¡
ğ‘–
ğ‘š
ğ‘’
_
ğ‘”
ğ‘
ğ‘
dh=
time_gap
h
2
	â€‹

âˆ’h
1
	â€‹

	â€‹


Vá»›i k = 1..(time_gap-1):

ğ‘¥
ğ‘¡
1
+
ğ‘˜
=
ğ‘¥
1
+
ğ‘˜
â‹…
ğ‘‘
ğ‘¥
x
t1+k
	â€‹

=x
1
	â€‹

+kâ‹…dx

ğ‘¦
ğ‘¡
1
+
ğ‘˜
=
ğ‘¦
1
+
ğ‘˜
â‹…
ğ‘‘
ğ‘¦
y
t1+k
	â€‹

=y
1
	â€‹

+kâ‹…dy

ğ‘¤
ğ‘¡
1
+
ğ‘˜
=
ğ‘¤
1
+
ğ‘˜
â‹…
ğ‘‘
ğ‘¤
w
t1+k
	â€‹

=w
1
	â€‹

+kâ‹…dw

â„
ğ‘¡
1
+
ğ‘˜
=
â„
1
+
ğ‘˜
â‹…
ğ‘‘
â„
h
t1+k
	â€‹

=h
1
	â€‹

+kâ‹…dh

Sau Ä‘Ã³: 
ğ‘ 
=
ğ‘¤
â‹…
â„
,
Â 
ğ‘Ÿ
=
ğ‘¤
/
â„
s=wâ‹…h,Â r=w/h

Code:

from typing import List

def interpolate_virtual_boxes(z1: np.ndarray, z2: np.ndarray, t1: int, t2: int, max_gap:int=50) -> List[np.ndarray]:
    """
    Return list of virtual measurements between z1@t1 and z2@t2 (exclusive).
    z1, z2: arrays shape (4,) as (x,y,s,r)
    returns: [z_hat_t1+1, ..., z_hat_t2-1] each shape (4,)
    """
    time_gap = int(t2 - t1)
    if time_gap <= 1:
        return []

    if time_gap > max_gap:
        # Avoid creating lots of virtual boxes if gap too large
        return []

    x1, y1, s1, r1 = map(float, z1)
    x2, y2, s2, r2 = map(float, z2)

    w1, h1 = s_r_to_w_h(s1, r1)
    w2, h2 = s_r_to_w_h(s2, r2)

    dx = (x2 - x1) / time_gap
    dy = (y2 - y1) / time_gap
    dw = (w2 - w1) / time_gap
    dh = (h2 - h1) / time_gap

    virtual_boxes = []
    for k in range(1, time_gap):
        xv = x1 + k * dx
        yv = y1 + k * dy
        wv = w1 + k * dw
        hv = h1 + k * dh
        sv, rv = w_h_to_s_r(wv, hv)
        virtual_boxes.append(np.array([xv, yv, sv, rv], dtype=float))
    return virtual_boxes

3) Appearance check (cosine similarity) â€” code

StrongSORT Ä‘Ã£ cÃ³ embedding extractor; mÃ¬nh dÃ¹ng má»™t hÃ m cosine similarity Ä‘Æ¡n giáº£n:

def cosine_similarity_vec(a: np.ndarray, b: np.ndarray, eps=1e-8):
    # a, b: 1D vectors
    a = a.reshape(-1)
    b = b.reshape(-1)
    na = np.linalg.norm(a) + eps
    nb = np.linalg.norm(b) + eps
    return float(np.dot(a, b) / (na * nb))

4) HÃ m chÃ­nh: apply_virtual_trajectory â€” tÃ­ch há»£p vÃ o pipeline

HÃ m nÃ y lÃ m viá»‡c ngoáº¡i vi (khÃ´ng báº¯t buá»™c thay Ä‘á»•i class KF), nhÆ°ng dÃ¹ng API kf.predict() vÃ  kf.update(meas, confidence=...) mÃ  KalmanFilterXYSR báº¡n Ä‘Ã£ cÃ³.

Ã tÆ°á»Ÿng:

táº¡o virtual list báº±ng interpolate_virtual_boxes

cho má»—i z_hat: náº¿u cÃ³ frame Ä‘á»ƒ crop + extract_feature hook thÃ¬ tÃ­nh sim vá»›i feature cá»§a z2; dá»±a trÃªn sim quyáº¿t Ä‘á»‹nh confidence vÃ  cÃ³ thÃªm hay khÃ´ng box áº£o vÃ o káº¿t quáº£ track.

LuÃ´n dÃ¹ng confidence tháº¥p cho measurement áº£o (conf_virtual), nhÆ°ng náº¿u appearance ráº¥t giá»‘ng (sim high), nÃ¢ng confidence (mapsimâ†’conf) hoáº·c mark virtual as accepted cho output.

Code Ä‘áº§y Ä‘á»§:

from typing import Callable, Optional, Tuple, Dict, Any

def apply_virtual_trajectory(
    kf,                       # KalmanFilterXYSR instance
    track,                    # track object or dict to which we may add boxes (user-provided)
    z1: np.ndarray, t1: int,   # last real measurement and time before miss
    z2: np.ndarray, t2: int,   # new real measurement and time after re-detect
    frames: Optional[Dict[int, Any]] = None,      # mapping frame_idx -> frame image (optional)
    crop_fn: Optional[Callable] = None,           # function(frame, x,y,w,h) -> crop image
    extract_feature: Optional[Callable] = None,   # function(crop) -> 1D embedding
    theta_sim: float = 0.65,
    conf_virtual_default: float = 0.30,
    conf_virtual_if_sim_high: Tuple[float,float]=(0.6, 0.9),  # mapping range for sim->conf
    max_gap: int = 30,
    add_virtual_to_track: Optional[Callable] = None, # function(track, box, virtual_flag)
):
    """
    Apply virtual trajectory between (z1,t1) and (z2,t2).
    - kf: KalmanFilterXYSR with methods predict(), update(meas, confidence=float)
    - track: user track object (can be dict), used only if add_virtual_to_track provided
    - frames + crop_fn + extract_feature: optional for appearance checking
    - add_virtual_to_track: optional callback to append virtual boxes to track result
    Returns:
      accepted_virtuals: list of (frame_idx, z_hat, sim) that were accepted by appearance
    """
    accepted_virtuals = []
    time_gap = int(t2 - t1)
    if time_gap <= 1 or time_gap > max_gap:
        # no intermediate frames or gap too large -> just update kf with z2
        kf.update(z2, confidence=1.0)
        if add_virtual_to_track:
            add_virtual_to_track(track, z2, virtual=False)
        return accepted_virtuals

    # create virtual boxes
    virtual_boxes = interpolate_virtual_boxes(z1, z2, t1, t2, max_gap=max_gap)

    # compute appearance feature of observed z2 if extractor available
    Fo = None
    if frames is not None and crop_fn is not None and extract_feature is not None and (t2 in frames):
        x2, y2, s2, r2 = z2
        w2, h2 = s_r_to_w_h(s2, r2)
        crop_obs = crop_fn(frames[t2], x2, y2, w2, h2)
        Fo = extract_feature(crop_obs)

    # iterate virtual boxes
    for idx, z_hat in enumerate(virtual_boxes, start=1):
        frame_idx = t1 + idx
        # appearance check if possible
        sim = None
        if Fo is not None and frames is not None and crop_fn is not None and extract_feature is not None and (frame_idx in frames):
            xh, yh, sh, rh = z_hat
            wh, hh = s_r_to_w_h(sh, rh)
            crop_v = crop_fn(frames[frame_idx], xh, yh, wh, hh)
            Fv = extract_feature(crop_v)
            sim = cosine_similarity_vec(Fo, Fv)

        # decide confidence
        if sim is None:
            conf = conf_virtual_default
            accept_for_output = False
        else:
            # map sim -> confidence, and decide acceptance
            if sim >= theta_sim:
                # high similarity -> stronger confidence and accept
                # simple mapping: conf = conf_virtual_if_sim_high[0] + (sim-theta)*(range)
                low, high = conf_virtual_if_sim_high
                # normalize sim in [theta_sim, 1]
                frac = min(1.0, max(0.0, (sim - theta_sim) / (1.0 - theta_sim + 1e-8)))
                conf = low + frac * (high - low)
                accept_for_output = True
            else:
                # low sim => very low confidence (only nudge KF), not accepted in output
                conf = min(conf_virtual_default, 0.2)
                accept_for_output = False

        # update KF with the virtual measurement (measurement is in x,y,s,r)
        kf.update(z_hat, confidence=conf)

        # If accepted -> optionally record into track / output
        if accept_for_output and add_virtual_to_track is not None:
            add_virtual_to_track(track, z_hat, virtual=True)
            accepted_virtuals.append((frame_idx, z_hat, sim))

        # if not last virtual, predict forward to next timestep
        if idx != len(virtual_boxes):
            kf.predict()

    # finally update with real observation
    kf.update(z2, confidence=1.0)
    if add_virtual_to_track is not None:
        add_virtual_to_track(track, z2, virtual=False)

    return accepted_virtuals


Giáº£i thÃ­ch Ä‘iá»ƒm quan trá»ng trong code:

virtual_boxes Ä‘Æ°á»£c sinh báº±ng ná»™i suy w,h (xem hÃ m interpolate_virtual_boxes).

Má»—i virtual z_hat Ä‘Æ°á»£c dÃ¹ng Ä‘á»ƒ gá»i kf.update(z_hat, confidence=conf); Ä‘iá»u nÃ y cho phÃ©p KF â€œnudgeâ€ dá»c Ä‘Æ°á»ng thay vÃ¬ nháº£y tá»« t1 â†’ t2.

conf Ä‘Æ°á»£c scale theo similarity (náº¿u cÃ³). Khi khÃ´ng cÃ³ appearance info, sá»­ dá»¥ng conf_virtual_default nhá» (vÃ­ dá»¥ 0.3).

add_virtual_to_track lÃ  callback cá»§a báº¡n (tÃ¹y pipeline) Ä‘á»ƒ thÃªm box vÃ o Ä‘áº§u ra náº¿u accept_for_output==True. Náº¿u báº¡n muá»‘n track luÃ´n xuáº¥t virtual boxes mÃ  khÃ´ng check appearance, set add_virtual_to_track vÃ  conf_virtual_default phÃ¹ há»£p.

5) Cáº­p nháº­t ná»™i dung unfreeze() trong KalmanFilterXYSR

Báº¡n Ä‘Ã£ cÃ³ má»™t unfreeze() trong class. MÃ¬nh khuyáº¿n nghá»‹ sá»­a Ä‘á»ƒ unfreeze() gá»i update(new_box, confidence=conf_virtual) thay vÃ¬ máº·c Ä‘á»‹nh 1.0. DÆ°á»›i Ä‘Ã¢y lÃ  Ã½ chÃ­nh (patch):

# inside KalmanFilterXYSR.unfreeze(), replace the block that calls self.update(new_box)
# currently: self.update(new_box)
# change to:
conf_virtual_default = 0.30  # tune as needed
self.update(new_box, confidence=conf_virtual_default)


Náº¿u báº¡n muá»‘n há»— trá»£ appearance inside-class, báº¡n cÃ³ thá»ƒ cung cáº¥p callback hoáº·c property self.extract_feature_fn vÃ  frames buffer self.frame_buffer Ä‘á»ƒ unfreeze() cÃ³ thá»ƒ compute sim vÃ  chá»n confidence adaptively (tÆ°Æ¡ng tá»± apply_virtual_trajectory).

6) CÃ¡c Ä‘iá»ƒm cáº§n chÃº Ã½ / tuning cho video ná»™i soi (biáº¿n dáº¡ng máº¡nh)

max_gap (tá»‘i Ä‘a bÆ°á»›c Ä‘á»ƒ táº¡o virtual): ná»™i soi cÃ³ biáº¿n dáº¡ng lá»›n â†’ chá»n nhá» (3..8 frames). Náº¿u framerate cao báº¡n cÃ³ thá»ƒ tÄƒng nháº¹.

conf_virtual_default: 0.2â€“0.4 (áº£o) â€” cÃ ng nhá» cÃ ng â€œmá»mâ€, nhÆ°ng náº¿u quÃ¡ nhá» KF sáº½ khÃ´ng Ä‘Æ°á»£c hiá»‡u chá»‰nh Ä‘á»§.

theta_sim: náº¿u extractor embedding á»•n â†’ 0.65â€“0.8; náº¿u embedding yáº¿u (Ä‘á»‘i tÆ°á»£ng thay Ä‘á»•i texture nhiá»u), giáº£m xuá»‘ng 0.45â€“0.6.

Náº¿u tá»•n thÆ°Æ¡ng biáº¿n dáº¡ng phi tuyáº¿n nhiá»u (co giÃ£n, shape change), tuyáº¿n tÃ­nh w,h váº«n lÃ  approximation â€” báº¡n cÃ³ thá»ƒ dÃ¹ng spline hoáº·c constant-acceleration model, nhÆ°ng thÆ°á»ng chi phÃ­/Ä‘á»™ phá»©c táº¡p tÄƒng.

Náº¿u báº¡n khÃ´ng cÃ³ frames cÅ© Ä‘á»ƒ crop (hoáº·c memory háº¡n cháº¿), váº«n dÃ¹ng virtual boxes Ä‘á»ƒ update KF (khÃ´ng appearance check). Äiá»u nÃ y váº«n tá»‘t Ä‘á»ƒ giáº£m ID-switch khi gap ngáº¯n.

Sau viá»‡c Ã¡p dá»¥ng virtual boxes, cÃ³ thá»ƒ cháº¡y temporal smoothing (EMA) lÃªn trajectory xuáº¥t ra Ä‘á»ƒ giáº£m jitter do update áº£o.

7) VÃ­ dá»¥ cáº¯m tháº³ng vÃ o StrongSORT (pseudo-integration)

Giáº£ sá»­ trong tracker loop khi báº¡n matched track tr with new detection det at frame t:

if tr.lost and det and (t - tr.last_seen_time) <= max_gap:
    # we have z1 at tr.last_seen_time, z2 is det at t
    z1 = tr.last_measurement  # (x,y,s,r)
    z2 = det.xy s r...
    accepted = apply_virtual_trajectory(
        kf=tr.kf, track=tr, z1=z1, t1=tr.last_seen_time,
        z2=z2, t2=t, frames=frame_buffer, crop_fn=crop_fn,
        extract_feature=extract_feature_fn, theta_sim=0.65,
        conf_virtual_default=0.30, add_virtual_to_track=tr.add_box
    )
else:
    # normal update
    tr.kf.update(det.measurement, confidence=det.confidence)

8) Kiá»ƒm tra tÃ­nh Ä‘Ãºng Ä‘áº¯n (test plan)

Unit-test interpolate_virtual_boxes vá»›i vÃ i z1,z2 Ä‘á»ƒ kiá»ƒm tra w,h->s,r chuyá»ƒn Ä‘á»•i.

Dry-run: simulate sequence: observe at t=0, miss t=1..3, observe again t=4 â†’ call apply_virtual_trajectory and log kf.x states before and after. Dá»± kiáº¿n: tráº¡ng thÃ¡i trÆ°á»›c sau mÆ°á»£t hÆ¡n (khÃ´ng jump).

A/B test: cháº¡y full video vá»›i/khÃ´ng dÃ¹ng virtual trajectory, so sÃ¡nh:

sá»‘ ID-switch, fragmentations, MOTA (náº¿u ground-truth).

quan sÃ¡t báº±ng máº¯t (visualize trajectories).

Tune theta_sim, conf_virtual_default, max_gap.

9) TÃ³m táº¯t ngáº¯n gá»n

Táº¡o virtual boxes báº±ng ná»™i suy w,h giá»¯a (x,y,s,r) hai quan sÃ¡t.

DÃ¹ng kf.update(z_hat, confidence=...) cho má»—i há»™p áº£o; confidence tháº¥p Ä‘á»ƒ biá»ƒu diá»…n Ä‘á»™ khÃ´ng cháº¯c cháº¯n.

Náº¿u cÃ³ appearance features, dÃ¹ng cosine similarity Ä‘á»ƒ quyáº¿t Ä‘á»‹nh cÃ³ cháº¥p nháº­n box áº£o vÃ o output hay chá»‰ dÃ¹ng nÃ³ Ä‘á»ƒ hiá»‡u chá»‰nh KF.

Háº¡n cháº¿ gap dÃ i, scale noise lá»›n, test vÃ  tune.