M√¥ t·∫£ chi ti·∫øt + code: t√≠ch h·ª£p qu·ªπ ƒë·∫°o ·∫£o v√†o KalmanFilterXYSR

D∆∞·ªõi ƒë√¢y m√¨nh cung c·∫•p m·ªôt h∆∞·ªõng tri·ªÉn khai ƒë·∫ßy ƒë·ªß (l√Ω thuy·∫øt ‚Üí c√¥ng th·ª©c ‚Üí code Python) ƒë·ªÉ b·∫°n c·∫Øm th·∫≥ng v√†o pipeline XYSR c·ªßa b·∫°n. N·ªôi dung g·ªìm:

C√°c ph√©p bi·∫øn ƒë·ªïi to√°n ‚Üí code (s,r ‚Üî w,h).

H√†m n·ªôi suy t·∫°o danh s√°ch bounding boxes ·∫£o (x, y, s, r) v·ªõi c√¥ng th·ª©c tuy·∫øn t√≠nh (n·ªôi suy w,h).

H√†m √°p d·ª•ng qu·ªπ ƒë·∫°o ·∫£o v√†o Kalman filter (apply_virtual_trajectory) ‚Äî th·ª±c hi·ªán: c·∫≠p nh·∫≠t KF t·ª´ng b∆∞·ªõc, d√πng confidence cho measurement ·∫£o, optional appearance check (cosine sim) ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ ‚Äúxu·∫•t‚Äù c√°c h·ªôp ·∫£o v√†o k·∫øt qu·∫£ tracking hay kh√¥ng.

G·ª£i √Ω ch·ªânh unfreeze() / update() trong class c·ªßa b·∫°n ƒë·ªÉ d√πng confidence khi update ·∫£o.

Ki·ªÉm tra v√† tham s·ªë tuning.

M√¨nh ghi r·∫•t r√µ c√¥ng th·ª©c to√°n trong comment ƒë·ªÉ b·∫°n d·ªÖ theo d√µi.

1) Chuy·ªÉn ƒë·ªïi s,r ‚Üî w,h (To√°n ‚Üí code)

C√¥ng th·ª©c:

t·ª´ s, r sang w, h:

ùë§
=
ùë†
‚ãÖ
ùëü
w=
s‚ãÖr
	‚Äã


‚Ñé
=
ùë†
/
ùëü
h=
s/r
	‚Äã


t·ª´ w, h sang s, r:

ùë†
=
ùë§
‚ãÖ
‚Ñé
s=w‚ãÖh

ùëü
=
ùë§
/
‚Ñé
r=w/h

Code:

import numpy as np

_eps = 1e-6

def s_r_to_w_h(s: float, r: float):
    """Convert scale s (area) and ratio r to width w and height h."""
    # numeric safety
    s = float(s)
    r = float(max(r, _eps))
    w = np.sqrt(max(s * r, 0.0))
    h = np.sqrt(max(s / r, 0.0))
    return w, h

def w_h_to_s_r(w: float, h: float):
    """Convert width and height to scale s and ratio r."""
    w = float(max(w, _eps))
    h = float(max(h, _eps))
    s = w * h
    r = w / h
    return s, r

2) N·ªôi suy tuy·∫øn t√≠nh cho qu·ªπ ƒë·∫°o ·∫£o (c√¥ng th·ª©c + code)

Gi·∫£ s·ª≠ c√≥ hai quan s√°t:

ùëß
1
=
(
ùë•
1
,
ùë¶
1
,
ùë†
1
,
ùëü
1
)
z
1
	‚Äã

=(x
1
	‚Äã

,y
1
	‚Äã

,s
1
	‚Äã

,r
1
	‚Äã

) t·∫°i frame t1

ùëß
2
=
(
ùë•
2
,
ùë¶
2
,
ùë†
2
,
ùëü
2
)
z
2
	‚Äã

=(x
2
	‚Äã

,y
2
	‚Äã

,s
2
	‚Äã

,r
2
	‚Äã

) t·∫°i frame t2

time_gap = t2 - t1 (s·ªë b∆∞·ªõc gi·ªØa hai quan s√°t; n·∫øu =1 th√¨ kh√¥ng c√≥ frame m·∫•t)

Chi ti·∫øt n·ªôi suy:

Chuy·ªÉn 
ùë†
ùëñ
,
ùëü
ùëñ
‚Üí
ùë§
ùëñ
,
‚Ñé
ùëñ
s
i
	‚Äã

,r
i
	‚Äã

‚Üíw
i
	‚Äã

,h
i
	‚Äã


T√≠nh delta t·ª´ng b∆∞·ªõc:

ùëë
ùë•
=
ùë•
2
‚àí
ùë•
1
ùë°
ùëñ
ùëö
ùëí
_
ùëî
ùëé
ùëù
dx=
time_gap
x
2
	‚Äã

‚àíx
1
	‚Äã

	‚Äã


ùëë
ùë¶
=
ùë¶
2
‚àí
ùë¶
1
ùë°
ùëñ
ùëö
ùëí
_
ùëî
ùëé
ùëù
dy=
time_gap
y
2
	‚Äã

‚àíy
1
	‚Äã

	‚Äã


ùëë
ùë§
=
ùë§
2
‚àí
ùë§
1
ùë°
ùëñ
ùëö
ùëí
_
ùëî
ùëé
ùëù
dw=
time_gap
w
2
	‚Äã

‚àíw
1
	‚Äã

	‚Äã


ùëë
‚Ñé
=
‚Ñé
2
‚àí
‚Ñé
1
ùë°
ùëñ
ùëö
ùëí
_
ùëî
ùëé
ùëù
dh=
time_gap
h
2
	‚Äã

‚àíh
1
	‚Äã

	‚Äã


V·ªõi k = 1..(time_gap-1):

ùë•
ùë°
1
+
ùëò
=
ùë•
1
+
ùëò
‚ãÖ
ùëë
ùë•
x
t1+k
	‚Äã

=x
1
	‚Äã

+k‚ãÖdx

ùë¶
ùë°
1
+
ùëò
=
ùë¶
1
+
ùëò
‚ãÖ
ùëë
ùë¶
y
t1+k
	‚Äã

=y
1
	‚Äã

+k‚ãÖdy

ùë§
ùë°
1
+
ùëò
=
ùë§
1
+
ùëò
‚ãÖ
ùëë
ùë§
w
t1+k
	‚Äã

=w
1
	‚Äã

+k‚ãÖdw

‚Ñé
ùë°
1
+
ùëò
=
‚Ñé
1
+
ùëò
‚ãÖ
ùëë
‚Ñé
h
t1+k
	‚Äã

=h
1
	‚Äã

+k‚ãÖdh

Sau ƒë√≥: 
ùë†
=
ùë§
‚ãÖ
‚Ñé
,
¬†
ùëü
=
ùë§
/
‚Ñé
s=w‚ãÖh,¬†r=w/h

Code:

from typing import List

def interpolate_virtual_boxes(z1: np.ndarray, z2: np.ndarray, t1: int, t2: int, max_gap:int=50) -> List[np.ndarray]:
    """
    Return list of virtual measurements between z1@t1 and z2@t2 (exclusive).
    z1, z2: arrays shape (4,) as (x,y,s,r)
    returns: [z_hat_t1+1, ..., z_hat_t2-1] each shape (4,)
    """
    time_gap = int(t2 - t1)
    if time_gap <= 1:
        return []

    if time_gap > max_gap:
        # Avoid creating lots of virtual boxes if gap too large
        return []

    x1, y1, s1, r1 = map(float, z1)
    x2, y2, s2, r2 = map(float, z2)

    w1, h1 = s_r_to_w_h(s1, r1)
    w2, h2 = s_r_to_w_h(s2, r2)

    dx = (x2 - x1) / time_gap
    dy = (y2 - y1) / time_gap
    dw = (w2 - w1) / time_gap
    dh = (h2 - h1) / time_gap

    virtual_boxes = []
    for k in range(1, time_gap):
        xv = x1 + k * dx
        yv = y1 + k * dy
        wv = w1 + k * dw
        hv = h1 + k * dh
        sv, rv = w_h_to_s_r(wv, hv)
        virtual_boxes.append(np.array([xv, yv, sv, rv], dtype=float))
    return virtual_boxes

3) Appearance check (cosine similarity) ‚Äî code

StrongSORT ƒë√£ c√≥ embedding extractor; m√¨nh d√πng m·ªôt h√†m cosine similarity ƒë∆°n gi·∫£n:

def cosine_similarity_vec(a: np.ndarray, b: np.ndarray, eps=1e-8):
    # a, b: 1D vectors
    a = a.reshape(-1)
    b = b.reshape(-1)
    na = np.linalg.norm(a) + eps
    nb = np.linalg.norm(b) + eps
    return float(np.dot(a, b) / (na * nb))

4) H√†m ch√≠nh: apply_virtual_trajectory ‚Äî t√≠ch h·ª£p v√†o pipeline

H√†m n√†y l√†m vi·ªác ngo·∫°i vi (kh√¥ng b·∫Øt bu·ªôc thay ƒë·ªïi class KF), nh∆∞ng d√πng API kf.predict() v√† kf.update(meas, confidence=...) m√† KalmanFilterXYSR b·∫°n ƒë√£ c√≥.

√ù t∆∞·ªüng:

t·∫°o virtual list b·∫±ng interpolate_virtual_boxes

cho m·ªói z_hat: n·∫øu c√≥ frame ƒë·ªÉ crop + extract_feature hook th√¨ t√≠nh sim v·ªõi feature c·ªßa z2; d·ª±a tr√™n sim quy·∫øt ƒë·ªãnh confidence v√† c√≥ th√™m hay kh√¥ng box ·∫£o v√†o k·∫øt qu·∫£ track.

Lu√¥n d√πng confidence th·∫•p cho measurement ·∫£o (conf_virtual), nh∆∞ng n·∫øu appearance r·∫•t gi·ªëng (sim high), n√¢ng confidence (mapsim‚Üíconf) ho·∫∑c mark virtual as accepted cho output.

Code ƒë·∫ßy ƒë·ªß:

from typing import Callable, Optional, Tuple, Dict, Any

def apply_virtual_trajectory(
    kf,                       # KalmanFilterXYSR instance
    track,                    # track object or dict to which we may add boxes (user-provided)
    z1: np.ndarray, t1: int,   # last real measurement and time before miss
    z2: np.ndarray, t2: int,   # new real measurement and time after re-detect
    frames: Optional[Dict[int, Any]] = None,      # mapping frame_idx -> frame image (optional)
    crop_fn: Optional[Callable] = None,           # function(frame, x,y,w,h) -> crop image
    extract_feature: Optional[Callable] = None,   # function(crop) -> 1D embedding
    theta_sim: float = 0.65,
    conf_virtual_default: float = 0.30,
    conf_virtual_if_sim_high: Tuple[float,float]=(0.6, 0.9),  # mapping range for sim->conf
    max_gap: int = 30,
    add_virtual_to_track: Optional[Callable] = None, # function(track, box, virtual_flag)
):
    """
    Apply virtual trajectory between (z1,t1) and (z2,t2).
    - kf: KalmanFilterXYSR with methods predict(), update(meas, confidence=float)
    - track: user track object (can be dict), used only if add_virtual_to_track provided
    - frames + crop_fn + extract_feature: optional for appearance checking
    - add_virtual_to_track: optional callback to append virtual boxes to track result
    Returns:
      accepted_virtuals: list of (frame_idx, z_hat, sim) that were accepted by appearance
    """
    accepted_virtuals = []
    time_gap = int(t2 - t1)
    if time_gap <= 1 or time_gap > max_gap:
        # no intermediate frames or gap too large -> just update kf with z2
        kf.update(z2, confidence=1.0)
        if add_virtual_to_track:
            add_virtual_to_track(track, z2, virtual=False)
        return accepted_virtuals

    # create virtual boxes
    virtual_boxes = interpolate_virtual_boxes(z1, z2, t1, t2, max_gap=max_gap)

    # compute appearance feature of observed z2 if extractor available
    Fo = None
    if frames is not None and crop_fn is not None and extract_feature is not None and (t2 in frames):
        x2, y2, s2, r2 = z2
        w2, h2 = s_r_to_w_h(s2, r2)
        crop_obs = crop_fn(frames[t2], x2, y2, w2, h2)
        Fo = extract_feature(crop_obs)

    # iterate virtual boxes
    for idx, z_hat in enumerate(virtual_boxes, start=1):
        frame_idx = t1 + idx
        # appearance check if possible
        sim = None
        if Fo is not None and frames is not None and crop_fn is not None and extract_feature is not None and (frame_idx in frames):
            xh, yh, sh, rh = z_hat
            wh, hh = s_r_to_w_h(sh, rh)
            crop_v = crop_fn(frames[frame_idx], xh, yh, wh, hh)
            Fv = extract_feature(crop_v)
            sim = cosine_similarity_vec(Fo, Fv)

        # decide confidence
        if sim is None:
            conf = conf_virtual_default
            accept_for_output = False
        else:
            # map sim -> confidence, and decide acceptance
            if sim >= theta_sim:
                # high similarity -> stronger confidence and accept
                # simple mapping: conf = conf_virtual_if_sim_high[0] + (sim-theta)*(range)
                low, high = conf_virtual_if_sim_high
                # normalize sim in [theta_sim, 1]
                frac = min(1.0, max(0.0, (sim - theta_sim) / (1.0 - theta_sim + 1e-8)))
                conf = low + frac * (high - low)
                accept_for_output = True
            else:
                # low sim => very low confidence (only nudge KF), not accepted in output
                conf = min(conf_virtual_default, 0.2)
                accept_for_output = False

        # update KF with the virtual measurement (measurement is in x,y,s,r)
        kf.update(z_hat, confidence=conf)

        # If accepted -> optionally record into track / output
        if accept_for_output and add_virtual_to_track is not None:
            add_virtual_to_track(track, z_hat, virtual=True)
            accepted_virtuals.append((frame_idx, z_hat, sim))

        # if not last virtual, predict forward to next timestep
        if idx != len(virtual_boxes):
            kf.predict()

    # finally update with real observation
    kf.update(z2, confidence=1.0)
    if add_virtual_to_track is not None:
        add_virtual_to_track(track, z2, virtual=False)

    return accepted_virtuals


Gi·∫£i th√≠ch ƒëi·ªÉm quan tr·ªçng trong code:

virtual_boxes ƒë∆∞·ª£c sinh b·∫±ng n·ªôi suy w,h (xem h√†m interpolate_virtual_boxes).

M·ªói virtual z_hat ƒë∆∞·ª£c d√πng ƒë·ªÉ g·ªçi kf.update(z_hat, confidence=conf); ƒëi·ªÅu n√†y cho ph√©p KF ‚Äúnudge‚Äù d·ªçc ƒë∆∞·ªùng thay v√¨ nh·∫£y t·ª´ t1 ‚Üí t2.

conf ƒë∆∞·ª£c scale theo similarity (n·∫øu c√≥). Khi kh√¥ng c√≥ appearance info, s·ª≠ d·ª•ng conf_virtual_default nh·ªè (v√≠ d·ª• 0.3).

add_virtual_to_track l√† callback c·ªßa b·∫°n (t√πy pipeline) ƒë·ªÉ th√™m box v√†o ƒë·∫ßu ra n·∫øu accept_for_output==True. N·∫øu b·∫°n mu·ªën track lu√¥n xu·∫•t virtual boxes m√† kh√¥ng check appearance, set add_virtual_to_track v√† conf_virtual_default ph√π h·ª£p.

5) C·∫≠p nh·∫≠t n·ªôi dung unfreeze() trong KalmanFilterXYSR

B·∫°n ƒë√£ c√≥ m·ªôt unfreeze() trong class. M√¨nh khuy·∫øn ngh·ªã s·ª≠a ƒë·ªÉ unfreeze() g·ªçi update(new_box, confidence=conf_virtual) thay v√¨ m·∫∑c ƒë·ªãnh 1.0. D∆∞·ªõi ƒë√¢y l√† √Ω ch√≠nh (patch):

# inside KalmanFilterXYSR.unfreeze(), replace the block that calls self.update(new_box)
# currently: self.update(new_box)
# change to:
conf_virtual_default = 0.30  # tune as needed
self.update(new_box, confidence=conf_virtual_default)


N·∫øu b·∫°n mu·ªën h·ªó tr·ª£ appearance inside-class, b·∫°n c√≥ th·ªÉ cung c·∫•p callback ho·∫∑c property self.extract_feature_fn v√† frames buffer self.frame_buffer ƒë·ªÉ unfreeze() c√≥ th·ªÉ compute sim v√† ch·ªçn confidence adaptively (t∆∞∆°ng t·ª± apply_virtual_trajectory).

6) C√°c ƒëi·ªÉm c·∫ßn ch√∫ √Ω / tuning cho video n·ªôi soi (bi·∫øn d·∫°ng m·∫°nh)

max_gap (t·ªëi ƒëa b∆∞·ªõc ƒë·ªÉ t·∫°o virtual): n·ªôi soi c√≥ bi·∫øn d·∫°ng l·ªõn ‚Üí ch·ªçn nh·ªè (3..8 frames). N·∫øu framerate cao b·∫°n c√≥ th·ªÉ tƒÉng nh·∫π.

conf_virtual_default: 0.2‚Äì0.4 (·∫£o) ‚Äî c√†ng nh·ªè c√†ng ‚Äúm·ªÅm‚Äù, nh∆∞ng n·∫øu qu√° nh·ªè KF s·∫Ω kh√¥ng ƒë∆∞·ª£c hi·ªáu ch·ªânh ƒë·ªß.

theta_sim: n·∫øu extractor embedding ·ªïn ‚Üí 0.65‚Äì0.8; n·∫øu embedding y·∫øu (ƒë·ªëi t∆∞·ª£ng thay ƒë·ªïi texture nhi·ªÅu), gi·∫£m xu·ªëng 0.45‚Äì0.6.

N·∫øu t·ªïn th∆∞∆°ng bi·∫øn d·∫°ng phi tuy·∫øn nhi·ªÅu (co gi√£n, shape change), tuy·∫øn t√≠nh w,h v·∫´n l√† approximation ‚Äî b·∫°n c√≥ th·ªÉ d√πng spline ho·∫∑c constant-acceleration model, nh∆∞ng th∆∞·ªùng chi ph√≠/ƒë·ªô ph·ª©c t·∫°p tƒÉng.

N·∫øu b·∫°n kh√¥ng c√≥ frames c≈© ƒë·ªÉ crop (ho·∫∑c memory h·∫°n ch·∫ø), v·∫´n d√πng virtual boxes ƒë·ªÉ update KF (kh√¥ng appearance check). ƒêi·ªÅu n√†y v·∫´n t·ªët ƒë·ªÉ gi·∫£m ID-switch khi gap ng·∫Øn.

Sau vi·ªác √°p d·ª•ng virtual boxes, c√≥ th·ªÉ ch·∫°y temporal smoothing (EMA) l√™n trajectory xu·∫•t ra ƒë·ªÉ gi·∫£m jitter do update ·∫£o.

7) V√≠ d·ª• c·∫Øm th·∫≥ng v√†o StrongSORT (pseudo-integration)

Gi·∫£ s·ª≠ trong tracker loop khi b·∫°n matched track tr with new detection det at frame t:

if tr.lost and det and (t - tr.last_seen_time) <= max_gap:
    # we have z1 at tr.last_seen_time, z2 is det at t
    z1 = tr.last_measurement  # (x,y,s,r)
    z2 = det.xy s r...
    accepted = apply_virtual_trajectory(
        kf=tr.kf, track=tr, z1=z1, t1=tr.last_seen_time,
        z2=z2, t2=t, frames=frame_buffer, crop_fn=crop_fn,
        extract_feature=extract_feature_fn, theta_sim=0.65,
        conf_virtual_default=0.30, add_virtual_to_track=tr.add_box
    )
else:
    # normal update
    tr.kf.update(det.measurement, confidence=det.confidence)

8) Ki·ªÉm tra t√≠nh ƒë√∫ng ƒë·∫Øn (test plan)

Unit-test interpolate_virtual_boxes v·ªõi v√†i z1,z2 ƒë·ªÉ ki·ªÉm tra w,h->s,r chuy·ªÉn ƒë·ªïi.

Dry-run: simulate sequence: observe at t=0, miss t=1..3, observe again t=4 ‚Üí call apply_virtual_trajectory and log kf.x states before and after. D·ª± ki·∫øn: tr·∫°ng th√°i tr∆∞·ªõc sau m∆∞·ª£t h∆°n (kh√¥ng jump).

A/B test: ch·∫°y full video v·ªõi/kh√¥ng d√πng virtual trajectory, so s√°nh:

s·ªë ID-switch, fragmentations, MOTA (n·∫øu ground-truth).

quan s√°t b·∫±ng m·∫Øt (visualize trajectories).

Tune theta_sim, conf_virtual_default, max_gap.

9) T√≥m t·∫Øt ng·∫Øn g·ªçn

T·∫°o virtual boxes b·∫±ng n·ªôi suy w,h gi·ªØa (x,y,s,r) hai quan s√°t.

D√πng kf.update(z_hat, confidence=...) cho m·ªói h·ªôp ·∫£o; confidence th·∫•p ƒë·ªÉ bi·ªÉu di·ªÖn ƒë·ªô kh√¥ng ch·∫Øc ch·∫Øn.

N·∫øu c√≥ appearance features, d√πng cosine similarity ƒë·ªÉ quy·∫øt ƒë·ªãnh c√≥ ch·∫•p nh·∫≠n box ·∫£o v√†o output hay ch·ªâ d√πng n√≥ ƒë·ªÉ hi·ªáu ch·ªânh KF.

H·∫°n ch·∫ø gap d√†i, scale noise l·ªõn, test v√† tune.